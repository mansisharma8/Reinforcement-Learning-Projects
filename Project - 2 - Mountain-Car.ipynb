{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b329f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09799016",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')#to make new gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05888000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40670149,  0.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()#parameters of given environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f373757a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space #params of o/p or things we can do with environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653d9203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33da7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for t in range(1000):\n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e54664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 20 episodes are over\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):#episode\n",
    "    observation = env.reset()\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation,reward,done,other_info = env.step(action)\n",
    "        if done:\n",
    "            #game episode over \n",
    "            print(\"Game episode {}/{} highscore :{}\".format(e,20,t))\n",
    "            break\n",
    "env.close()\n",
    "print(\"All 20 episodes are over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dc92bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8419f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size): #state_size ->i/p , action_size-> o/p\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95 #0.95 is the discount factor\n",
    "        self.epsilon = 1.0#explosion rate : how much to act randomly\n",
    "        self.epsilon_decay = 0.995  #decaying the epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._create_model()\n",
    "    def _create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim=self.state_size,activation='relu')) #1st Hidden Layer\n",
    "        model.add(Dense(24,activation='relu')) #2nd Hidden Layer\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    def act(self,state):\n",
    "        # Exploration vs Exploitation\n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state) # predict reward value based upon current state\n",
    "        return np.argmax(act_values[0]) #predict reward value based upon current\n",
    "    def train(self,batch_size=32): #method that trains NN with experiences sampled from memory\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for state,action,reward,next_state,done in minibatch:\n",
    "            \n",
    "            if not done: #boolean \n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0) #single epoch, x =state, y = target_f, loss--> target_f - \n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5f70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 50\n",
    "output_dir = \"car_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4ba4c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mansi\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=4,action_size=2)\n",
    "done = False\n",
    "state_size = 2\n",
    "action_size =3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "314df3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode :0/50, High Score:199,Exploration Rate:1.0\n",
      "Game Episode :1/50, High Score:199,Exploration Rate:0.99\n",
      "Game Episode :2/50, High Score:199,Exploration Rate:0.99\n",
      "Game Episode :3/50, High Score:199,Exploration Rate:0.99\n",
      "Game Episode :4/50, High Score:199,Exploration Rate:0.98\n",
      "Game Episode :5/50, High Score:199,Exploration Rate:0.98\n",
      "Game Episode :6/50, High Score:199,Exploration Rate:0.97\n",
      "Game Episode :7/50, High Score:199,Exploration Rate:0.97\n",
      "Game Episode :8/50, High Score:199,Exploration Rate:0.96\n",
      "Game Episode :9/50, High Score:199,Exploration Rate:0.96\n",
      "Game Episode :10/50, High Score:199,Exploration Rate:0.95\n",
      "Game Episode :11/50, High Score:199,Exploration Rate:0.95\n",
      "Game Episode :12/50, High Score:199,Exploration Rate:0.94\n",
      "Game Episode :13/50, High Score:199,Exploration Rate:0.94\n",
      "Game Episode :14/50, High Score:199,Exploration Rate:0.93\n",
      "Game Episode :15/50, High Score:199,Exploration Rate:0.93\n",
      "Game Episode :16/50, High Score:199,Exploration Rate:0.92\n",
      "Game Episode :17/50, High Score:199,Exploration Rate:0.92\n",
      "Game Episode :18/50, High Score:199,Exploration Rate:0.91\n",
      "Game Episode :19/50, High Score:199,Exploration Rate:0.91\n",
      "Game Episode :20/50, High Score:199,Exploration Rate:0.9\n",
      "Game Episode :21/50, High Score:199,Exploration Rate:0.9\n",
      "Game Episode :22/50, High Score:199,Exploration Rate:0.9\n",
      "Game Episode :23/50, High Score:199,Exploration Rate:0.89\n",
      "Game Episode :24/50, High Score:199,Exploration Rate:0.89\n",
      "Game Episode :25/50, High Score:199,Exploration Rate:0.88\n",
      "Game Episode :26/50, High Score:199,Exploration Rate:0.88\n",
      "Game Episode :27/50, High Score:199,Exploration Rate:0.87\n",
      "Game Episode :28/50, High Score:199,Exploration Rate:0.87\n",
      "Game Episode :29/50, High Score:199,Exploration Rate:0.86\n",
      "Game Episode :30/50, High Score:199,Exploration Rate:0.86\n",
      "Game Episode :31/50, High Score:199,Exploration Rate:0.86\n",
      "Game Episode :32/50, High Score:199,Exploration Rate:0.85\n",
      "Game Episode :33/50, High Score:199,Exploration Rate:0.85\n",
      "Game Episode :34/50, High Score:199,Exploration Rate:0.84\n",
      "Game Episode :35/50, High Score:199,Exploration Rate:0.84\n",
      "Game Episode :36/50, High Score:199,Exploration Rate:0.83\n",
      "Game Episode :37/50, High Score:199,Exploration Rate:0.83\n",
      "Game Episode :38/50, High Score:199,Exploration Rate:0.83\n",
      "Game Episode :39/50, High Score:199,Exploration Rate:0.82\n",
      "Game Episode :40/50, High Score:199,Exploration Rate:0.82\n",
      "Game Episode :41/50, High Score:199,Exploration Rate:0.81\n",
      "Game Episode :42/50, High Score:199,Exploration Rate:0.81\n",
      "Game Episode :43/50, High Score:199,Exploration Rate:0.81\n",
      "Game Episode :44/50, High Score:199,Exploration Rate:0.8\n",
      "Game Episode :45/50, High Score:199,Exploration Rate:0.8\n",
      "Game Episode :46/50, High Score:199,Exploration Rate:0.79\n",
      "Game Episode :47/50, High Score:199,Exploration Rate:0.79\n",
      "Game Episode :48/50, High Score:199,Exploration Rate:0.79\n",
      "Game Episode :49/50, High Score:199,Exploration Rate:0.78\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size, action_size) # initialise agent\n",
    "done = False\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,[1,state_size])\n",
    "    \n",
    "    for time in range(5000):\n",
    "        env.render()\n",
    "        action = agent.act(state) #action is 0 or 1\n",
    "        next_state,reward,done,other_info = env.step(action) \n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"Game Episode :{}/{}, High Score:{},Exploration Rate:{:.2}\".format(e,n_episodes,time,agent.epsilon))\n",
    "            break\n",
    "            \n",
    "    if len(agent.memory)>batch_size:\n",
    "        agent.train(batch_size)\n",
    "    \n",
    "    if e%50==0:\n",
    "        agent.save(output_dir+\"weights_\"+'{:04d}'.format(e)+\".hdf5\")\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078321c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
